{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Oqd4y5orF8h"
   },
   "source": [
    "<img src=\"https://www.digitalhouse.com/logo-DH.png\" width=\"200\" height=\"100\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogFYft1srF8i"
   },
   "source": [
    "<h3><b>Curso:</b> Data Science / <b>Año:</b> 2020 / <b>Sede:</b> Casa</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5E3bYYforF8i"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dn7OMfFurF8j"
   },
   "source": [
    "<h3><b>TP Integrador:</b> Text Mining de <i>tweets</i> de anuncios del gobierno durante la cuarentena.</h3>\n",
    "<blockquote>\n",
    "        <ul>\n",
    "          <li><i>Sentiment analysis</i> de los comentarios de los usuarios.</li>\n",
    "          <li>Clustering de <i>tweets</i> de los usuarios.</li>\n",
    "        </ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHhEZxazrF8j"
   },
   "source": [
    "<h3><b>Grupo 10:</b></h3>\n",
    "<blockquote>\n",
    "        <ul>\n",
    "          <li>Maria Eugenia Perotti</li>\n",
    "          <li>Gastón Ortíz</li>\n",
    "        </ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a los tweets, vamos a tratar de predecir el sentimiento respecto de la cuarentena con las siguientes etiquetas:\n",
    "* Positivo = A favor de la cuarentena.\n",
    "* Negativo = En contra de la cuarentena.\n",
    "* Neutral = No emite opinión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de libraries y funciones.\n",
    "## Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YvXbE3orF8l"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.cluster import DBSCAN,KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, make_union, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from spacy import displacy\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de funciones.\n",
    "En esta sección definimos las funciones que vamos a utilizar en la notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para Tokenizar los datos con spaCy\n",
    "\n",
    "Usaremos esta función para eliminar automáticamente la información que no necesitamos, como palabras vacías y puntuación, de cada tweet.\n",
    "\n",
    "Primero importamos los modelos de Español y el módulo de string de Python, que contiene una lista de todos los signos de puntuación que podemos usar en `string.punctuation`. Crearemos variables que contienen los signos de puntuación y las palabras vacías que queremos eliminar, y un analizador que ejecuta la entrada a través del módulo de español de spaCy.\n",
    "\n",
    "Luego, crearemos una función `spacy_tokenizer()` que acepta una oración como entrada y procesa la oración en _tokens_, realizando lematización, pasando todo a minúsculas y eliminando palabras vacías. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestra lista de signos de puntuación.\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Creamos nuestra lista de stopwords\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "stop_words = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "# Cargamos el tokenizador, tagger, parser, NER y los word vectors del Español.\n",
    "parser = Spanish()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creamos nuestro objeto token, que va a ser utilizado para crear documentos con anotaciones lingüísticas.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lematizamos cada token y lo pasamos a minúsculas.\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Sacamos las stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # retorna la lista de tokens preprocesada\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de un Transformer custom.\n",
    "Para continuar limpiando nuestros datos, creamos un transformador personalizado para eliminar los espacios iniciales y finales y convertir el texto en minúsculas. Aquí, crearemos una clase personalizada `predictors` que hereda de la clase `TransformerMixin`. Esta clase anula los métodos transform, fit y get_parrams. También crearemos una función `clean_text()` que elimina los espacios y convierte el texto en minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Feature Engineering (TF-IDF)\n",
    "Generamos una matriz de Bag of words para nuestros datos de texto utilizando `CountVectorizer` de scikit-learn y le decimos que use nuestra función `spacy_tokenizer` como su tokenizador, y definiendo el rango de n-gramas que queremos.\n",
    "\n",
    "El parámetro `ngram_range` establece los límites inferior y superior de nuestros n-gramas (usaremos unigrams). Luego asignaremos los n-gramas a una variable bow_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También nos interesa mirar el **TF-IDF** de nuestros términos, ya que podemosa saber qué tan importante es un término en particular en el contexto de un documento dado, basado en cuántas veces aparece el término y en cuántos otros tweets aparece ese mismo término. Cuanto mayor sea el **TF-IDF**, más importante es ese término para ese tweet.\n",
    "\n",
    "Generamos el TF-IDF automáticamente usando `TfidfVectorizer` de scikit-learn. Nuevamente, le diremos que use el tokenizador personalizado que creamos con spaCy, y luego asignaremos el resultado a la variable `tfidf_vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impresión de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección definiremos las funciones a reutilizar para imprimir los resultados de las prediciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para mostrar las Métricas de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset de todos los tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el dataset con todos los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6014, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>fecha</th>\n",
       "      <th>anuncio</th>\n",
       "      <th>ubicacion</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nerinapf</td>\n",
       "      <td>por favor la gente que esta en la calle y tien...</td>\n",
       "      <td>2020-03-19 23:58:22+00:00</td>\n",
       "      <td>Anuncio_1</td>\n",
       "      <td>Buenos Aires, Argentina</td>\n",
       "      <td>1240789742494580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MileParamo</td>\n",
       "      <td>todxs sin coronavirus pero con hantavirus esa ...</td>\n",
       "      <td>2020-03-19 23:57:59+00:00</td>\n",
       "      <td>Anuncio_1</td>\n",
       "      <td>Buenos Aires, Argentina</td>\n",
       "      <td>1240789649938960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maiferretti</td>\n",
       "      <td>no puedo creer q sigan tomandose en joda lo de...</td>\n",
       "      <td>2020-03-19 23:56:53+00:00</td>\n",
       "      <td>Anuncio_1</td>\n",
       "      <td>Buenos Aires, Argentina</td>\n",
       "      <td>1240789371130910000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      username                                              tweet  \\\n",
       "0     Nerinapf  por favor la gente que esta en la calle y tien...   \n",
       "1   MileParamo  todxs sin coronavirus pero con hantavirus esa ...   \n",
       "2  Maiferretti  no puedo creer q sigan tomandose en joda lo de...   \n",
       "\n",
       "                       fecha    anuncio                ubicacion  \\\n",
       "0  2020-03-19 23:58:22+00:00  Anuncio_1  Buenos Aires, Argentina   \n",
       "1  2020-03-19 23:57:59+00:00  Anuncio_1  Buenos Aires, Argentina   \n",
       "2  2020-03-19 23:56:53+00:00  Anuncio_1  Buenos Aires, Argentina   \n",
       "\n",
       "                    id  \n",
       "0  1240789742494580000  \n",
       "1  1240789649938960000  \n",
       "2  1240789371130910000  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sin_clasificar = pd.read_csv('Data/all_tweets.csv', sep=';')\n",
    "print(df_sin_clasificar.shape)\n",
    "df_sin_clasificar.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sin_clasificar['sentimiento'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el dataset con los tweets ya clasificados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(807, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>fecha</th>\n",
       "      <th>anuncio</th>\n",
       "      <th>ubicacion</th>\n",
       "      <th>id</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VozdeRosario</td>\n",
       "      <td>coronavirus suman 5527 los fallecidos y 282437...</td>\n",
       "      <td>2020-08-14 23:36:23+00:00</td>\n",
       "      <td>Anuncio_11</td>\n",
       "      <td>Rosario, Argentina</td>\n",
       "      <td>1294417616153560000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VozdeRosario</td>\n",
       "      <td>un arquero de la seleccion argentina fue diagn...</td>\n",
       "      <td>2020-08-14 23:18:04+00:00</td>\n",
       "      <td>Anuncio_11</td>\n",
       "      <td>Rosario, Argentina</td>\n",
       "      <td>1294413007334800000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VozdeRosario</td>\n",
       "      <td>coronavirus en rosario 77 nuevos casos en la c...</td>\n",
       "      <td>2020-08-14 23:13:34+00:00</td>\n",
       "      <td>Anuncio_11</td>\n",
       "      <td>Rosario, Argentina</td>\n",
       "      <td>1294411876374380000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       username                                              tweet  \\\n",
       "0  VozdeRosario  coronavirus suman 5527 los fallecidos y 282437...   \n",
       "1  VozdeRosario  un arquero de la seleccion argentina fue diagn...   \n",
       "2  VozdeRosario  coronavirus en rosario 77 nuevos casos en la c...   \n",
       "\n",
       "                       fecha     anuncio           ubicacion  \\\n",
       "0  2020-08-14 23:36:23+00:00  Anuncio_11  Rosario, Argentina   \n",
       "1  2020-08-14 23:18:04+00:00  Anuncio_11  Rosario, Argentina   \n",
       "2  2020-08-14 23:13:34+00:00  Anuncio_11  Rosario, Argentina   \n",
       "\n",
       "                    id sentimiento  \n",
       "0  1294417616153560000     Neutral  \n",
       "1  1294413007334800000     Neutral  \n",
       "2  1294411876374380000     Neutral  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clasificados = pd.read_csv('Data/usuarios_mas_10_tweets_clasificados.csv', sep=';')\n",
    "print(df_clasificados.shape)\n",
    "df_clasificados.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a impactar las clasificaciones al dataset completo con un merge por `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tweets clasificados en el dataset, previo al merge: 0\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de tweets clasificados en el dataset, previo al merge:',df_todos.sentimiento.notnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sin_clasificar = df_sin_clasificar.set_index('id')\n",
    "df_clasificados = df_clasificados.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_sin_clasificar, df_clasificados, how='left', on=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que el merge haya sido exitoso, verificamos que el número de no nulos sea el mismo que la cantidad de registros en el dataset de clasificados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.sentimiento_y.notnull().shape[0] == df_clasificados.shape[0]\n",
    "#Arreglar esto fijandome en los vacíos también"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username_x</th>\n",
       "      <th>tweet_x</th>\n",
       "      <th>username_y</th>\n",
       "      <th>tweet_y</th>\n",
       "      <th>sentimiento_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1240786072533180000</th>\n",
       "      <td>gustavorearte1</td>\n",
       "      <td>el presidente alberto fernandez tras reunirse ...</td>\n",
       "      <td>gustavorearte1</td>\n",
       "      <td>el presidente alberto fernandez tras reunirse ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240784152456230000</th>\n",
       "      <td>gustavorearte1</td>\n",
       "      <td>confirmaron 31 nuevos casos de coronavirus en ...</td>\n",
       "      <td>gustavorearte1</td>\n",
       "      <td>confirmaron 31 nuevos casos de coronavirus en ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240775792377930000</th>\n",
       "      <td>mnspezzapria</td>\n",
       "      <td>coronavirus la cumbre se hace en el quincho de...</td>\n",
       "      <td>mnspezzapria</td>\n",
       "      <td>coronavirus la cumbre se hace en el quincho de...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240775699990020000</th>\n",
       "      <td>AvellanedaReal</td>\n",
       "      <td>coronavirus al borde de los 10000 muertos por ...</td>\n",
       "      <td>AvellanedaReal</td>\n",
       "      <td>coronavirus al borde de los 10000 muertos por ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240770138330270000</th>\n",
       "      <td>SerLeal_</td>\n",
       "      <td>lo mas terrible es que podria ser cierto se im...</td>\n",
       "      <td>SerLeal_</td>\n",
       "      <td>lo mas terrible es que podria ser cierto se im...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294105579586280000</th>\n",
       "      <td>con_sello</td>\n",
       "      <td>coronavirus el gobierno de chubut prohibio el ...</td>\n",
       "      <td>con_sello</td>\n",
       "      <td>coronavirus el gobierno de chubut prohibio el ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294100069185090000</th>\n",
       "      <td>con_sello</td>\n",
       "      <td>coronavirus en el dia de hoy se dio a conocer ...</td>\n",
       "      <td>con_sello</td>\n",
       "      <td>coronavirus en el dia de hoy se dio a conocer ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294069183907600000</th>\n",
       "      <td>ADNsur</td>\n",
       "      <td>comodoro suma otros 10 casos de coronavirus y ...</td>\n",
       "      <td>ADNsur</td>\n",
       "      <td>comodoro suma otros 10 casos de coronavirus y ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294063178566040000</th>\n",
       "      <td>con_sello</td>\n",
       "      <td>coronavirus otras 149 personas murieron y 7498...</td>\n",
       "      <td>con_sello</td>\n",
       "      <td>coronavirus otras 149 personas murieron y 7498...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294061838209090000</th>\n",
       "      <td>ADNsur</td>\n",
       "      <td>hugo sigman el empresario responsable de elabo...</td>\n",
       "      <td>ADNsur</td>\n",
       "      <td>hugo sigman el empresario responsable de elabo...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>807 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         username_x  \\\n",
       "id                                    \n",
       "1240786072533180000  gustavorearte1   \n",
       "1240784152456230000  gustavorearte1   \n",
       "1240775792377930000    mnspezzapria   \n",
       "1240775699990020000  AvellanedaReal   \n",
       "1240770138330270000        SerLeal_   \n",
       "...                             ...   \n",
       "1294105579586280000       con_sello   \n",
       "1294100069185090000       con_sello   \n",
       "1294069183907600000          ADNsur   \n",
       "1294063178566040000       con_sello   \n",
       "1294061838209090000          ADNsur   \n",
       "\n",
       "                                                               tweet_x  \\\n",
       "id                                                                       \n",
       "1240786072533180000  el presidente alberto fernandez tras reunirse ...   \n",
       "1240784152456230000  confirmaron 31 nuevos casos de coronavirus en ...   \n",
       "1240775792377930000  coronavirus la cumbre se hace en el quincho de...   \n",
       "1240775699990020000  coronavirus al borde de los 10000 muertos por ...   \n",
       "1240770138330270000  lo mas terrible es que podria ser cierto se im...   \n",
       "...                                                                ...   \n",
       "1294105579586280000  coronavirus el gobierno de chubut prohibio el ...   \n",
       "1294100069185090000  coronavirus en el dia de hoy se dio a conocer ...   \n",
       "1294069183907600000  comodoro suma otros 10 casos de coronavirus y ...   \n",
       "1294063178566040000  coronavirus otras 149 personas murieron y 7498...   \n",
       "1294061838209090000  hugo sigman el empresario responsable de elabo...   \n",
       "\n",
       "                         username_y  \\\n",
       "id                                    \n",
       "1240786072533180000  gustavorearte1   \n",
       "1240784152456230000  gustavorearte1   \n",
       "1240775792377930000    mnspezzapria   \n",
       "1240775699990020000  AvellanedaReal   \n",
       "1240770138330270000        SerLeal_   \n",
       "...                             ...   \n",
       "1294105579586280000       con_sello   \n",
       "1294100069185090000       con_sello   \n",
       "1294069183907600000          ADNsur   \n",
       "1294063178566040000       con_sello   \n",
       "1294061838209090000          ADNsur   \n",
       "\n",
       "                                                               tweet_y  \\\n",
       "id                                                                       \n",
       "1240786072533180000  el presidente alberto fernandez tras reunirse ...   \n",
       "1240784152456230000  confirmaron 31 nuevos casos de coronavirus en ...   \n",
       "1240775792377930000  coronavirus la cumbre se hace en el quincho de...   \n",
       "1240775699990020000  coronavirus al borde de los 10000 muertos por ...   \n",
       "1240770138330270000  lo mas terrible es que podria ser cierto se im...   \n",
       "...                                                                ...   \n",
       "1294105579586280000  coronavirus el gobierno de chubut prohibio el ...   \n",
       "1294100069185090000  coronavirus en el dia de hoy se dio a conocer ...   \n",
       "1294069183907600000  comodoro suma otros 10 casos de coronavirus y ...   \n",
       "1294063178566040000  coronavirus otras 149 personas murieron y 7498...   \n",
       "1294061838209090000  hugo sigman el empresario responsable de elabo...   \n",
       "\n",
       "                    sentimiento_y  \n",
       "id                                 \n",
       "1240786072533180000       Neutral  \n",
       "1240784152456230000       Neutral  \n",
       "1240775792377930000       Neutral  \n",
       "1240775699990020000       Neutral  \n",
       "1240770138330270000      Positivo  \n",
       "...                           ...  \n",
       "1294105579586280000       Neutral  \n",
       "1294100069185090000       Neutral  \n",
       "1294069183907600000       Neutral  \n",
       "1294063178566040000       Neutral  \n",
       "1294061838209090000       Neutral  \n",
       "\n",
       "[807 rows x 5 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.loc[df_merge.sentimiento_y.notnull(), ['username_x', 'tweet_x','username_y','tweet_y','sentimiento_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.sentimiento_y.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para usar el concat, tenemos que hacer un reset index\n",
    "#df_merge = pd.concat([df_sin_clasificar, df_clasificados[['sentimiento','id']]], join='outer', axis=1)\n",
    "#df_merge = df_sin_clasificar.set_index('id').join(df_clasificados[['sentimiento','id']].set_index('id'), lsuffix='_all', how='inner').rename_axis(index='indice').reset_index()\n",
    "\n",
    "\n",
    "#df_merge = pd.merge(df_sin_clasificar, df_clasificados[['sentimiento']], how='left')\n",
    "\n",
    "#print(df_merge.shape)\n",
    "#df_merge\n",
    "#df_clean.merge(sucursales, how = 'left', left_on='tcre_numero_sucursal', right_on='suc_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos a cuántos tweets se les impactó el valor de `sentimiento`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad de tweets clasificados en el dataset, post merge:',df_todos.sentimiento.notnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos cómo está distribuida la muestra que tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Distribución de sentimiento')\n",
    "df.sentimiento.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputación de clasificaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ya clasificado lo vamos a traer por ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividimos los datos en Training y Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweet']\n",
    "y = df['sentimiento']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del Pipeline y Generación del modelo.\n",
    "Ya podemos construir nuestro modelo con Multinomial Naive Bayes.\n",
    "\n",
    "Luego, crearemos un _pipeline_ con tres componentes: un limpiador, un vectorizador y un clasificador. El limpiador usa nuestra clase `predictors` para limpiar y preprocesar el texto. El vectorizador usa objetos `bow_vector` para crear la matriz de BoW para nuestro texto. El clasificador de por sí realiza la clasificación de los sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
